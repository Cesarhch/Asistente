import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# âœ… 1ï¸âƒ£ Liberar Memoria Antes de Cargar el Modelo
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()

# ğŸ“‚ Ruta del modelo base (Phi-3-mini-4k-instruct)
model_path = "D:/Phi-3-mini-4k-instruct"

# âœ… 2ï¸âƒ£ CuantizaciÃ³n en 4-bit con `float32`
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float32,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# ğŸ“¥ Cargar el modelo con 4-bit y optimizaciÃ³n de memoria
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float32 
)

# ğŸ“¥ Cargar el tokenizador
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# ğŸ”¹ **Asegurar que `use_cache=False`**
model.config.use_cache = False  

# âœ… 3ï¸âƒ£ Configurar LoRA con los mÃ³dulos correctos de Phi-3-mini-4k
lora_config = LoraConfig(
    r=8, 
    lora_alpha=32, 
    lora_dropout=0.1, #Dato de aleatoriedad, antes en 0.05 
    bias="none", 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# âœ… Habilitar `gradient_checkpointing` para ahorro de memoria
model.gradient_checkpointing_disable()

# âœ… 4ï¸âƒ£ Asegurar que solo los parÃ¡metros de LoRA sean entrenables
for param in model.parameters():
    param.requires_grad = False  

# ğŸ”¹ Activar solo los parÃ¡metros de LoRA
for name, param in model.named_parameters():
    if "lora" in name or "adapter" in name:
        param.requires_grad = True

# ğŸ”¥ Asegurar que el modelo estÃ© en modo entrenamiento
model.train()

# ğŸš€ Verificar si hay parÃ¡metros entrenables
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"ğŸ” ParÃ¡metros entrenables: {trainable_params} / {total_params}")

if trainable_params == 0:
    raise ValueError("âŒ ERROR: No hay parÃ¡metros entrenables en el modelo.")

print(f"âœ… Modelo cargado en: {model.device}")

# ğŸ“‚ Cargar dataset JSONL
dataset_path = r"D:/serveria/Asistente-main/Finetuning/dataset.jsonl"
dataset = load_dataset("json", data_files=dataset_path)["train"]

# ğŸ” Verificar estructura del dataset antes de tokenizar
print("Ejemplo de dataset antes de tokenizar:", dataset[0])

# ğŸ”  FunciÃ³n de TokenizaciÃ³n con Labels
def tokenize_function(examples):
    if "prompt" not in examples or "completion" not in examples:
        raise ValueError("âŒ Error: El dataset no tiene los campos 'prompt' y 'completion'.")

    combined_texts = [p + " " + c for p, c in zip(examples["prompt"], examples["completion"]) if p and c]
    
    if not combined_texts:
        raise ValueError("âŒ Error: Algunos ejemplos en el dataset estÃ¡n vacÃ­os o mal formateados.")

    tokenized = tokenizer(combined_texts, truncation=True, padding="max_length", max_length=100)
    
    # âœ… Agregar labels (necesarios para calcular loss)
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

# ğŸ“Œ Tokenizar dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# âš™ï¸ 5ï¸âƒ£ ConfiguraciÃ³n del Entrenamiento Optimizado
training_args = TrainingArguments(
    output_dir="D:/finetuned_phi3",
    per_device_train_batch_size=2,  
    gradient_accumulation_steps=4,  
    save_strategy="epoch",
    save_steps=50,
    logging_dir="D:/logs",
    logging_steps=5,
    max_steps=500,  
    fp16=True,
    evaluation_strategy="no",
    push_to_hub=False,
    learning_rate=5e-5  # ğŸ”¹ Ajustado para mejor precisiÃ³n
    #learning_rate=1e-4
)

# ğŸ”¹ **AÃ±adir `data_collator` optimizado**
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding="longest")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator  
)

# ğŸš€ Iniciar el Fine-Tuning
trainer.train()

# ğŸ“¥ Guardar el modelo fine-tuneado
save_path = "D:/finetuned_phi3"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"âœ… Modelo fine-tuneado guardado en: {save_path}")
